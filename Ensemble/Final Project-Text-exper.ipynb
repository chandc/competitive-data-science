{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project submitted by Daniel Chan, Feb 4, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check software versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.13.3\n",
      "pandas 0.20.3\n",
      "scipy 0.19.1\n",
      "sklearn 0.19.1\n",
      "lightgbm 2.0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "import lightgbm \n",
    "\n",
    "for p in [np, pd, scipy, sklearn, lightgbm]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv('../Final Project/sales_train.csv.gz')\n",
    "shops = pd.read_csv('../Final Project/shops.csv')\n",
    "items = pd.read_csv('../Final Project/items.csv')\n",
    "item_cats = pd.read_csv('../Final Project/item_categories.csv')\n",
    "ww = pd.read_csv('../Final Project/test.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate `test` file to the end of `sales` dataframe so we can calculate meta-features consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  shop_id  item_id\n",
      "0   0        5     5037\n",
      "1   1        5     5320\n",
      "2   2        5     5233\n",
      "3   3        5     5232\n",
      "4   4        5     5268\n"
     ]
    }
   ],
   "source": [
    "#print (sales.head(5))\n",
    "print (ww.head(5))\n",
    "del sales['date'] \n",
    "del sales['item_price']\n",
    "del ww['ID']\n",
    "ww['item_cnt_day'] = 0\n",
    "ww['date_block_num'] = 34\n",
    "\n",
    "sales = pd.concat([sales,ww],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add up the number of items sold for each month.  Use data statistics max, min and standard deviation as advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielchan/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# additional features derived from sales statistics: max, min and std \n",
    "\n",
    "# Same as above but with shop-item-month max\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target_max':np.max}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-item-month min\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target_min':np.min}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-item-month mean\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target_mean':np.std}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Expanding Mean Encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499605291823\n"
     ]
    }
   ],
   "source": [
    "globalmean = all_data['target'].mean()\n",
    "\n",
    "csum   = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\n",
    "cumcnt = all_data.groupby('item_id').cumcount()\n",
    "\n",
    "small = 0.\n",
    "\n",
    "all_data['Expanding Mean'] = csum/(cumcnt+small)\n",
    "all_data['Expanding Mean'].fillna(globalmean, inplace=True)    # Fill NaNs with global mean\n",
    "\n",
    "encoded_feature = all_data['Expanding Mean'].values\n",
    "\n",
    "\n",
    "corr = np.corrcoef(all_data['target'].values, encoded_feature)[0][1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11128050, 10)\n"
     ]
    }
   ],
   "source": [
    "print (all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1100528fe254fe4bb487eb4a913e4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_max</th>\n",
       "      <th>target_min</th>\n",
       "      <th>target_mean</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>Expanding Mean</th>\n",
       "      <th>Expanding Mean_lag_1</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_max_lag_1</th>\n",
       "      <th>target_mean_lag_1</th>\n",
       "      <th>target_min_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>Expanding Mean_lag_2</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_max_lag_2</th>\n",
       "      <th>target_mean_lag_2</th>\n",
       "      <th>target_min_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>Expanding Mean_lag_3</th>\n",
       "      <th>...</th>\n",
       "      <th>target_mean_lag_3</th>\n",
       "      <th>target_min_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>Expanding Mean_lag_4</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_max_lag_4</th>\n",
       "      <th>target_mean_lag_4</th>\n",
       "      <th>target_min_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>Expanding Mean_lag_5</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_max_lag_5</th>\n",
       "      <th>target_mean_lag_5</th>\n",
       "      <th>target_min_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>Expanding Mean_lag_12</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_max_lag_12</th>\n",
       "      <th>target_mean_lag_12</th>\n",
       "      <th>target_min_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>10297</td>\n",
       "      <td>12</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.483516</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>3.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>10296</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>10298</td>\n",
       "      <td>12</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881917</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>13.299270</td>\n",
       "      <td>14.136752</td>\n",
       "      <td>21.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.759555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>11.935065</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1309.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.582378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>2.12132</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6676.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>10300</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.416058</td>\n",
       "      <td>3.829060</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>3.727273</td>\n",
       "      <td>31.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.423893</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6676.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>10284</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.133772</td>\n",
       "      <td>0.135321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.151335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6676.0</td>\n",
       "      <td>0.166065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7827.0</td>\n",
       "      <td>0.160173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7792.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  target  target_max  target_min  \\\n",
       "0       54    10297              12     4.0         1.0         1.0   \n",
       "1       54    10296              12     3.0         1.0         1.0   \n",
       "2       54    10298              12    14.0         3.0         1.0   \n",
       "3       54    10300              12     3.0         1.0         1.0   \n",
       "4       54    10284              12     1.0         1.0         1.0   \n",
       "\n",
       "   target_mean  target_shop  target_item  Expanding Mean  \\\n",
       "0     0.000000       8198.0         23.0        0.483516   \n",
       "1     0.000000       8198.0         17.0        0.521739   \n",
       "2     0.881917       8198.0        182.0       13.299270   \n",
       "3     0.000000       8198.0         26.0        3.416058   \n",
       "4     0.000000       8198.0          3.0        0.133772   \n",
       "\n",
       "   Expanding Mean_lag_1  target_lag_1  target_item_lag_1  target_max_lag_1  \\\n",
       "0              0.408451           3.0               42.0               1.0   \n",
       "1              0.615385           0.0               24.0               0.0   \n",
       "2             14.136752          21.0              369.0               3.0   \n",
       "3              3.829060           1.0               54.0               1.0   \n",
       "4              0.135321           0.0                4.0               0.0   \n",
       "\n",
       "   target_mean_lag_1  target_min_lag_1  target_shop_lag_1  \\\n",
       "0           0.000000               1.0            10055.0   \n",
       "1           0.000000               0.0            10055.0   \n",
       "2           0.759555               1.0            10055.0   \n",
       "3           0.000000               1.0            10055.0   \n",
       "4           0.000000               0.0            10055.0   \n",
       "\n",
       "   Expanding Mean_lag_2  target_lag_2  target_item_lag_2  target_max_lag_2  \\\n",
       "0              0.064516           0.0                2.0               0.0   \n",
       "1              0.000000           0.0                0.0               0.0   \n",
       "2             11.935065         119.0             1309.0              15.0   \n",
       "3              3.727273          31.0              361.0               5.0   \n",
       "4              0.138889           0.0                3.0               0.0   \n",
       "\n",
       "   target_mean_lag_2  target_min_lag_2  target_shop_lag_2  \\\n",
       "0           0.000000               0.0             7978.0   \n",
       "1           0.000000               0.0                0.0   \n",
       "2           4.582378               1.0             7978.0   \n",
       "3           1.423893               1.0             7978.0   \n",
       "4           0.000000               0.0             7978.0   \n",
       "\n",
       "   Expanding Mean_lag_3        ...         target_mean_lag_3  \\\n",
       "0              0.000000        ...                   0.00000   \n",
       "1              0.000000        ...                   0.00000   \n",
       "2              3.166667        ...                   2.12132   \n",
       "3              0.944444        ...                   0.00000   \n",
       "4              0.151335        ...                   0.00000   \n",
       "\n",
       "   target_min_lag_3  target_shop_lag_3  Expanding Mean_lag_4  target_lag_4  \\\n",
       "0               0.0                0.0              0.000000           0.0   \n",
       "1               0.0                0.0              0.000000           0.0   \n",
       "2               2.0             6676.0              0.000000           0.0   \n",
       "3               0.0             6676.0              0.000000           0.0   \n",
       "4               0.0             6676.0              0.166065           0.0   \n",
       "\n",
       "   target_item_lag_4  target_max_lag_4  target_mean_lag_4  target_min_lag_4  \\\n",
       "0                0.0               0.0                0.0               0.0   \n",
       "1                0.0               0.0                0.0               0.0   \n",
       "2                0.0               0.0                0.0               0.0   \n",
       "3                0.0               0.0                0.0               0.0   \n",
       "4                3.0               0.0                0.0               0.0   \n",
       "\n",
       "   target_shop_lag_4  Expanding Mean_lag_5  target_lag_5  target_item_lag_5  \\\n",
       "0                0.0              0.000000           0.0                0.0   \n",
       "1                0.0              0.000000           0.0                0.0   \n",
       "2                0.0              0.000000           0.0                0.0   \n",
       "3                0.0              0.000000           0.0                0.0   \n",
       "4             7827.0              0.160173           0.0               10.0   \n",
       "\n",
       "   target_max_lag_5  target_mean_lag_5  target_min_lag_5  target_shop_lag_5  \\\n",
       "0               0.0                0.0               0.0                0.0   \n",
       "1               0.0                0.0               0.0                0.0   \n",
       "2               0.0                0.0               0.0                0.0   \n",
       "3               0.0                0.0               0.0                0.0   \n",
       "4               0.0                0.0               0.0             7792.0   \n",
       "\n",
       "   Expanding Mean_lag_12  target_lag_12  target_item_lag_12  \\\n",
       "0                    0.0            0.0                 0.0   \n",
       "1                    0.0            0.0                 0.0   \n",
       "2                    0.0            0.0                 0.0   \n",
       "3                    0.0            0.0                 0.0   \n",
       "4                    0.0            0.0                 0.0   \n",
       "\n",
       "   target_max_lag_12  target_mean_lag_12  target_min_lag_12  \\\n",
       "0                0.0                 0.0                0.0   \n",
       "1                0.0                 0.0                0.0   \n",
       "2                0.0                 0.0                0.0   \n",
       "3                0.0                 0.0                0.0   \n",
       "4                0.0                 0.0                0.0   \n",
       "\n",
       "   target_shop_lag_12  item_category_id  \n",
       "0                 0.0                37  \n",
       "1                 0.0                38  \n",
       "2                 0.0                40  \n",
       "3                 0.0                37  \n",
       "4                 0.0                57  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Text features using TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6639294, 53)\n"
     ]
    }
   ],
   "source": [
    "#make a copy of all_data before processing\n",
    "\n",
    "#all_data_check = all_data.copy()\n",
    "print (all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#all_data = all_data_check.copy()\n",
    "all_data = pd.merge(all_data,item_cats,how='left',on='item_category_id')\n",
    "\n",
    "# focus on the top 5 key words: dvd, game, cinema, cinema dvd, music and gifts\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 2), min_df=0.15)\n",
    "# Fit transform both train & test data to get tf_idf_features\n",
    "tf_idf_features = tf_idf.fit_transform(all_data.item_category_name.values)\n",
    "\n",
    "#all_data = pd.concat([all_data, pd.DataFrame(tf_idf_features[0: all_data.shape[0]].toarray())], axis=1)\n",
    "all_data = pd.concat([all_data, pd.DataFrame(tf_idf_features.toarray()], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the `all_data` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_max</th>\n",
       "      <th>target_min</th>\n",
       "      <th>target_mean</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>Expanding Mean</th>\n",
       "      <th>Expanding Mean_lag_1</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_max_lag_1</th>\n",
       "      <th>target_mean_lag_1</th>\n",
       "      <th>target_min_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>Expanding Mean_lag_2</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_max_lag_2</th>\n",
       "      <th>target_mean_lag_2</th>\n",
       "      <th>target_min_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>Expanding Mean_lag_3</th>\n",
       "      <th>...</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_max_lag_4</th>\n",
       "      <th>target_mean_lag_4</th>\n",
       "      <th>target_min_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>Expanding Mean_lag_5</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_max_lag_5</th>\n",
       "      <th>target_mean_lag_5</th>\n",
       "      <th>target_min_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>Expanding Mean_lag_12</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_max_lag_12</th>\n",
       "      <th>target_mean_lag_12</th>\n",
       "      <th>target_min_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>10297</td>\n",
       "      <td>12</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.483516</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>3.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>10296</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>10298</td>\n",
       "      <td>12</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881917</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>13.299270</td>\n",
       "      <td>14.136752</td>\n",
       "      <td>21.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.759555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>11.935065</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1309.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.582378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482008</td>\n",
       "      <td>0.619544</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>10300</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.416058</td>\n",
       "      <td>3.829060</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>3.727273</td>\n",
       "      <td>31.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.423893</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>10284</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.133772</td>\n",
       "      <td>0.135321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.151335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7827.0</td>\n",
       "      <td>0.160173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7792.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  target  target_max  target_min  \\\n",
       "0       54    10297              12     4.0         1.0         1.0   \n",
       "1       54    10296              12     3.0         1.0         1.0   \n",
       "2       54    10298              12    14.0         3.0         1.0   \n",
       "3       54    10300              12     3.0         1.0         1.0   \n",
       "4       54    10284              12     1.0         1.0         1.0   \n",
       "\n",
       "   target_mean  target_shop  target_item  Expanding Mean  \\\n",
       "0     0.000000       8198.0         23.0        0.483516   \n",
       "1     0.000000       8198.0         17.0        0.521739   \n",
       "2     0.881917       8198.0        182.0       13.299270   \n",
       "3     0.000000       8198.0         26.0        3.416058   \n",
       "4     0.000000       8198.0          3.0        0.133772   \n",
       "\n",
       "   Expanding Mean_lag_1  target_lag_1  target_item_lag_1  target_max_lag_1  \\\n",
       "0              0.408451           3.0               42.0               1.0   \n",
       "1              0.615385           0.0               24.0               0.0   \n",
       "2             14.136752          21.0              369.0               3.0   \n",
       "3              3.829060           1.0               54.0               1.0   \n",
       "4              0.135321           0.0                4.0               0.0   \n",
       "\n",
       "   target_mean_lag_1  target_min_lag_1  target_shop_lag_1  \\\n",
       "0           0.000000               1.0            10055.0   \n",
       "1           0.000000               0.0            10055.0   \n",
       "2           0.759555               1.0            10055.0   \n",
       "3           0.000000               1.0            10055.0   \n",
       "4           0.000000               0.0            10055.0   \n",
       "\n",
       "   Expanding Mean_lag_2  target_lag_2  target_item_lag_2  target_max_lag_2  \\\n",
       "0              0.064516           0.0                2.0               0.0   \n",
       "1              0.000000           0.0                0.0               0.0   \n",
       "2             11.935065         119.0             1309.0              15.0   \n",
       "3              3.727273          31.0              361.0               5.0   \n",
       "4              0.138889           0.0                3.0               0.0   \n",
       "\n",
       "   target_mean_lag_2  target_min_lag_2  target_shop_lag_2  \\\n",
       "0           0.000000               0.0             7978.0   \n",
       "1           0.000000               0.0                0.0   \n",
       "2           4.582378               1.0             7978.0   \n",
       "3           1.423893               1.0             7978.0   \n",
       "4           0.000000               0.0             7978.0   \n",
       "\n",
       "   Expanding Mean_lag_3 ...   target_lag_4  target_item_lag_4  \\\n",
       "0              0.000000 ...            0.0                0.0   \n",
       "1              0.000000 ...            0.0                0.0   \n",
       "2              3.166667 ...            0.0                0.0   \n",
       "3              0.944444 ...            0.0                0.0   \n",
       "4              0.151335 ...            0.0                3.0   \n",
       "\n",
       "   target_max_lag_4  target_mean_lag_4  target_min_lag_4  target_shop_lag_4  \\\n",
       "0               0.0                0.0               0.0                0.0   \n",
       "1               0.0                0.0               0.0                0.0   \n",
       "2               0.0                0.0               0.0                0.0   \n",
       "3               0.0                0.0               0.0                0.0   \n",
       "4               0.0                0.0               0.0             7827.0   \n",
       "\n",
       "   Expanding Mean_lag_5  target_lag_5  target_item_lag_5  target_max_lag_5  \\\n",
       "0              0.000000           0.0                0.0               0.0   \n",
       "1              0.000000           0.0                0.0               0.0   \n",
       "2              0.000000           0.0                0.0               0.0   \n",
       "3              0.000000           0.0                0.0               0.0   \n",
       "4              0.160173           0.0               10.0               0.0   \n",
       "\n",
       "   target_mean_lag_5  target_min_lag_5  target_shop_lag_5  \\\n",
       "0                0.0               0.0                0.0   \n",
       "1                0.0               0.0                0.0   \n",
       "2                0.0               0.0                0.0   \n",
       "3                0.0               0.0                0.0   \n",
       "4                0.0               0.0             7792.0   \n",
       "\n",
       "   Expanding Mean_lag_12  target_lag_12  target_item_lag_12  \\\n",
       "0                    0.0            0.0                 0.0   \n",
       "1                    0.0            0.0                 0.0   \n",
       "2                    0.0            0.0                 0.0   \n",
       "3                    0.0            0.0                 0.0   \n",
       "4                    0.0            0.0                 0.0   \n",
       "\n",
       "   target_max_lag_12  target_mean_lag_12  target_min_lag_12  \\\n",
       "0                0.0                 0.0                0.0   \n",
       "1                0.0                 0.0                0.0   \n",
       "2                0.0                 0.0                0.0   \n",
       "3                0.0                 0.0                0.0   \n",
       "4                0.0                 0.0                0.0   \n",
       "\n",
       "   target_shop_lag_12         0    1         2         3    4  \n",
       "0                 0.0  0.000000  0.0  1.000000  0.000000  0.0  \n",
       "1                 0.0  0.000000  0.0  1.000000  0.000000  0.0  \n",
       "2                 0.0  0.619544  0.0  0.482008  0.619544  0.0  \n",
       "3                 0.0  0.000000  0.0  1.000000  0.000000  0.0  \n",
       "4                 0.0  0.000000  0.0  0.000000  0.000000  1.0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.drop(['item_category_name','item_category_id'], axis=1, inplace=True)\n",
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test. We will treat last month data as the test set. `Test` should really be called `validation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 33\n",
      "Submit `date_block_num` is 34\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "sub_block = dates.max()\n",
    "last_block = dates.max() - 1\n",
    "print('Test `date_block_num` is %d' % last_block)\n",
    "print('Submit `date_block_num` is %d' % sub_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train = dates[dates <  last_block]\n",
    "dates_test  = dates[dates == last_block]\n",
    "\n",
    "X_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n",
    "X_sub  =  all_data.loc[dates == sub_block].drop(to_drop_cols, axis=1)\n",
    "\n",
    "\n",
    "y_train = all_data.loc[dates <  last_block, 'target'].values\n",
    "y_test =  all_data.loc[dates == last_block, 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6186922, 49)\n",
      "(238172, 49)\n",
      "(214200, 49)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "\n",
    "print (X_test.shape)\n",
    "\n",
    "print (X_sub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First level models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at 3 different linear regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for linreg is 0.964911\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values, y_train)\n",
    "pred_lr = lr.predict(X_test.values)\n",
    "\n",
    "pred_lr = pred_lr.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print('Test RMSE for linreg is %f' % mean_squared_error(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for lassoReg is 0.991678\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lassoReg = Lasso(alpha=0.8)\n",
    "lassoReg.fit(X_train.values, y_train) \n",
    "pred_lasso = lassoReg.predict(X_test.values)\n",
    "\n",
    "pred_lasso = pred_lasso.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print('Test RMSE for lassoReg is %f' % mean_squared_error(y_test, pred_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=True, random_state=None, solver='auto', tol=0.001)\n",
      "Test RMSE for ridgeReg is 0.971817\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgeReg = Ridge(alpha=0.5,normalize=True )\n",
    "\n",
    "print (ridgeReg)\n",
    "ridgeReg.fit(X_train.values, y_train) \n",
    "pred_ridge = ridgeReg.predict(X_test.values)\n",
    "\n",
    "pred_ridge = pred_ridge.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print('Test RMSE for ridgeReg is %f' % mean_squared_error(y_test, pred_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the hyperparameters for LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_fraction is 0.400000\n",
      "Test RMSE for LightGBM is 0.934225\n",
      "feature_fraction is 0.433333\n",
      "Test RMSE for LightGBM is 0.928551\n",
      "feature_fraction is 0.466667\n",
      "Test RMSE for LightGBM is 0.923982\n",
      "feature_fraction is 0.500000\n",
      "Test RMSE for LightGBM is 0.925315\n",
      "feature_fraction is 0.533333\n",
      "Test RMSE for LightGBM is 0.923157\n",
      "feature_fraction is 0.566667\n",
      "Test RMSE for LightGBM is 0.924516\n",
      "feature_fraction is 0.600000\n",
      "Test RMSE for LightGBM is 0.928001\n",
      "feature_fraction is 0.633333\n",
      "Test RMSE for LightGBM is 0.929679\n",
      "feature_fraction is 0.666667\n",
      "Test RMSE for LightGBM is 0.931415\n",
      "feature_fraction is 0.700000\n",
      "Test RMSE for LightGBM is 0.935911\n"
     ]
    }
   ],
   "source": [
    "ffs_to_try = np.linspace(0.4, 0.7, 10)\n",
    "\n",
    "for ff in ffs_to_try:\n",
    "\n",
    "    print('feature_fraction is %f' % ff)\n",
    "\n",
    "\n",
    "    lgb_params = {\n",
    "               'feature_fraction': ff,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':8, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.01, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "    model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "    pred_lgb = model.predict(X_test)\n",
    "\n",
    "    pred_lgb = pred_lgb.clip(0,20.)\n",
    "    y_test = y_test.clip(0,20.)\n",
    "    print('Test RMSE for LightGBM is %f' % mean_squared_error(y_test, pred_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for LightGBM is 0.923157\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "               'feature_fraction': 0.533333,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':8, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.01, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "pred_lgb = model.predict(X_test)\n",
    "\n",
    "pred_lgb = pred_lgb.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "print('Test RMSE for LightGBM is %f' % mean_squared_error(y_test, pred_lgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenate test predictions to get test meta-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238172, 2)\n"
     ]
    }
   ],
   "source": [
    "X_test_level2 = np.c_[pred_lr, pred_lgb] \n",
    "\n",
    "\n",
    "print (X_test_level2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-Features for the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214200, 49)\n",
      "(214200, 2)\n"
     ]
    }
   ],
   "source": [
    "print (X_sub.shape)\n",
    "\n",
    "sub_lr = lr.predict(X_sub.values).clip(0,20.)\n",
    "#sub_lr = lassoReg.predict(X_sub.values).clip(0,20.)\n",
    "sub_lgb = model.predict(X_sub).clip(0,20.)\n",
    "\n",
    "X_sub_level2 = np.c_[sub_lr, sub_lgb] \n",
    "print (X_sub_level2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement ***scheme f)*** from the reading material. Here, we will use duration **T** equal to month and **M=15**.  \n",
    "\n",
    "We need to get predictions (meta-features) from *linear regression* and *LightGBM* for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "y_train_level2 = y_train_level2.clip(0,20.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n",
    "\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in [27, 28, 29, 30, 31, 32]:\n",
    "    \n",
    "    print(cur_block_num)\n",
    "    \n",
    "    Xtemp = X_train[dates_train < cur_block_num]\n",
    "    ytemp = y_train[dates_train < cur_block_num]\n",
    "    \n",
    "    Xtemp_1 = X_train[dates_train == cur_block_num]\n",
    "    \n",
    "    lr.fit(Xtemp.values, ytemp)\n",
    "    pred_lr = lr.predict(Xtemp_1.values)\n",
    "    pred_lr = pred_lr.clip(0,20.)\n",
    "    \n",
    "    model = lgb.train(lgb_params, lgb.Dataset(Xtemp, label=ytemp), 100)\n",
    "    pred_lgb = model.predict(Xtemp_1)\n",
    "    pred_lgb = pred_lgb.clip(0,20.)\n",
    "\n",
    "\n",
    "    #X_train_level2[dates_train_level2==cur_block_num,:2] = np.c_[pred_lr, pred_lgb]\n",
    "    X_train_level2[dates_train_level2==cur_block_num,:] = np.c_[pred_lr, pred_lgb]\n",
    "\n",
    "#print (X_train_level2.shape)\n",
    "#print(X_train_level2.mean(axis=0))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure there is sufficient diversity between the 2 meta models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89bcdb0780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_train_level2[:,0], X_train_level2[:,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the meta-features are created, we can ensemble our first level models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple linear convex mix:\n",
    "\n",
    "$$\n",
    "mix= \\alpha\\cdot\\text{linreg_prediction}+(1-\\alpha)\\cdot\\text{lgb_prediction}\n",
    "$$\n",
    "\n",
    "We need to find an optimal $\\alpha$ vy doing a grid search. Find the optimal $\\alpha$ out of `alphas_to_try` array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the training error for simple convex mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.347000; Corresponding RMSE score on train: 0.736529\n"
     ]
    }
   ],
   "source": [
    "alphas_to_try = np.linspace(0, 1, 1001)\n",
    "\n",
    "r2_train_simple_mix = 2.0\n",
    "\n",
    "for alpha in alphas_to_try:\n",
    "    mix = alpha*X_train_level2[:,0] + (1.0-alpha)*X_train_level2[:,1]   \n",
    "    mix = mix.clip(0,20.)\n",
    "    #print (np.amax(y_train_level2 - mix))\n",
    "\n",
    "    rtemp = mean_squared_error(y_train_level2, mix)\n",
    "    if (rtemp < r2_train_simple_mix) :\n",
    "        r2_train_simple_mix = rtemp\n",
    "        best_alpha = alpha\n",
    "        \n",
    "\n",
    "print('Best alpha: %f; Corresponding RMSE score on train: %f' % (best_alpha, r2_train_simple_mix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the testing (validation) error for simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the best $\\alpha$ to compute predictions for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238172, 2)\n",
      "(238172,)\n",
      "(238172,)\n",
      "Test RMSE for simple mix is 0.899241\n"
     ]
    }
   ],
   "source": [
    "test_preds = best_alpha*X_test_level2[:,0]  +  (1.0-best_alpha)*X_test_level2[:,1]\n",
    "\n",
    "test_preds = test_preds.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print (X_test_level2.shape)\n",
    "print (y_test.shape)\n",
    "print (test_preds.shape)\n",
    "\n",
    "r2_test_simple_mix = mean_squared_error(y_test, test_preds)     # YOUR CODE GOES HERE\n",
    "\n",
    "print('Test RMSE for simple mix is %f' % r2_test_simple_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear regression model to the meta-features. Use the same parameters as in the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "linReg = LinearRegression(normalize=False)\n",
    "linReg.fit(X_train_level2, y_train_level2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute RSME on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE for stacking is 0.730194\n",
      "Test  RMSE for stacking is 0.895259\n"
     ]
    }
   ],
   "source": [
    "train_preds = linReg.predict(X_train_level2)  \n",
    "train_preds = train_preds.clip(0,20.)                                       \n",
    "r2_train_stacking = mean_squared_error(y_train_level2, train_preds)     \n",
    "\n",
    "test_preds = linReg.predict(X_test_level2)              \n",
    "test_preds = test_preds.clip(0,20.)                                     \n",
    "\n",
    "r2_test_stacking = mean_squared_error(y_test, test_preds)   \n",
    "\n",
    "print('Train RMSE for stacking is %f' % r2_train_stacking)\n",
    "print('Test  RMSE for stacking is %f' % r2_test_stacking)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking method produced a lower Test RMSE than simple mix method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214200, 49)\n",
      "(214200,)\n",
      "(214200, 3)\n",
      "(214200, 51)\n"
     ]
    }
   ],
   "source": [
    "# submission using the stacking method\n",
    "sub_preds = linReg.predict(X_sub_level2)              \n",
    "sub_preds = sub_preds.clip(0,20.)  \n",
    "\n",
    "\n",
    "# submission using the simple mix method\n",
    "#sub_preds = best_alpha*X_sub_level2[:,0]  +  (1.0-best_alpha)*X_sub_level2[:,1]\n",
    "#sub_preds = sub_preds.clip(0,20.)\n",
    "\n",
    "\n",
    "print (X_sub.shape)\n",
    "print (sub_preds.shape)\n",
    "\n",
    "X_sub['target'] = sub_preds\n",
    "\n",
    "ww = pd.read_csv('../Final Project/test.csv.gz')\n",
    "\n",
    "\n",
    "submit = pd.merge(ww,X_sub,how='left',on=['shop_id', 'item_id']).fillna(0)\n",
    "\n",
    "print (ww.shape)\n",
    "print (submit.shape)\n",
    "\n",
    "#print (submit.head(10))\n",
    "#print (submit[submit.date_block_num == 0.0].count())\n",
    "\n",
    "submit.to_csv(path_or_buf='submit_max_min.std_Mean_Encoding_Text_stacking.csv',columns=['ID','target'], header=['ID', 'item_cnt_month'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
