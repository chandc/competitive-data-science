{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project submitted by Daniel Chan, Feb 4, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check software versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.13.3\n",
      "pandas 0.20.3\n",
      "scipy 0.19.1\n",
      "sklearn 0.19.1\n",
      "lightgbm 2.0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "import lightgbm \n",
    "\n",
    "for p in [np, pd, scipy, sklearn, lightgbm]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv('../Final Project/sales_train.csv.gz')\n",
    "shops = pd.read_csv('../Final Project/shops.csv')\n",
    "items = pd.read_csv('../Final Project/items.csv')\n",
    "item_cats = pd.read_csv('../Final Project/item_categories.csv')\n",
    "ww = pd.read_csv('../Final Project/test.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate `test` file to the end of `sales` dataframe so we can calculate meta-features consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  shop_id  item_id\n",
      "0   0        5     5037\n",
      "1   1        5     5320\n",
      "2   2        5     5233\n",
      "3   3        5     5232\n",
      "4   4        5     5268\n"
     ]
    }
   ],
   "source": [
    "#print (sales.head(5))\n",
    "print (ww.head(5))\n",
    "del sales['date'] \n",
    "del sales['item_price']\n",
    "del ww['ID']\n",
    "ww['item_cnt_day'] = 0\n",
    "ww['date_block_num'] = 34\n",
    "\n",
    "sales = pd.concat([sales,ww],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add up the number of items sold for each month.  Use data statistics max, min and standard deviation as advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielchan/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# additional features derived from sales statistics: max, min and std \n",
    "\n",
    "# Same as above but with shop-item-month max\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target_max':np.max}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-item-month min\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target_min':np.min}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-item-month mean\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target_mean':np.std}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Expanding Mean Encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499605291823\n"
     ]
    }
   ],
   "source": [
    "globalmean = all_data['target'].mean()\n",
    "\n",
    "csum   = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\n",
    "cumcnt = all_data.groupby('item_id').cumcount()\n",
    "\n",
    "small = 0.\n",
    "\n",
    "all_data['Expanding Mean'] = csum/(cumcnt+small)\n",
    "all_data['Expanding Mean'].fillna(globalmean, inplace=True)    # Fill NaNs with global mean\n",
    "\n",
    "encoded_feature = all_data['Expanding Mean'].values\n",
    "\n",
    "\n",
    "corr = np.corrcoef(all_data['target'].values, encoded_feature)[0][1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11128050, 10)\n"
     ]
    }
   ],
   "source": [
    "print (all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4c88f99ca94b1997a3eeb60df11bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the `all_data` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_max</th>\n",
       "      <th>target_min</th>\n",
       "      <th>target_mean</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>Expanding Mean</th>\n",
       "      <th>Expanding Mean_lag_1</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_max_lag_1</th>\n",
       "      <th>target_mean_lag_1</th>\n",
       "      <th>target_min_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>Expanding Mean_lag_2</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_max_lag_2</th>\n",
       "      <th>target_mean_lag_2</th>\n",
       "      <th>target_min_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>Expanding Mean_lag_3</th>\n",
       "      <th>...</th>\n",
       "      <th>target_mean_lag_3</th>\n",
       "      <th>target_min_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>Expanding Mean_lag_4</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_max_lag_4</th>\n",
       "      <th>target_mean_lag_4</th>\n",
       "      <th>target_min_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>Expanding Mean_lag_5</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_max_lag_5</th>\n",
       "      <th>target_mean_lag_5</th>\n",
       "      <th>target_min_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>Expanding Mean_lag_12</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_max_lag_12</th>\n",
       "      <th>target_mean_lag_12</th>\n",
       "      <th>target_min_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>10297</td>\n",
       "      <td>12</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.483516</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>3.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>10296</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>10298</td>\n",
       "      <td>12</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881917</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>13.299270</td>\n",
       "      <td>14.136752</td>\n",
       "      <td>21.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.759555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>11.935065</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1309.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.582378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>2.12132</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6676.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>10300</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.416058</td>\n",
       "      <td>3.829060</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>3.727273</td>\n",
       "      <td>31.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.423893</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6676.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>10284</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.133772</td>\n",
       "      <td>0.135321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.151335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6676.0</td>\n",
       "      <td>0.166065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7827.0</td>\n",
       "      <td>0.160173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7792.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  target  target_max  target_min  \\\n",
       "0       54    10297              12     4.0         1.0         1.0   \n",
       "1       54    10296              12     3.0         1.0         1.0   \n",
       "2       54    10298              12    14.0         3.0         1.0   \n",
       "3       54    10300              12     3.0         1.0         1.0   \n",
       "4       54    10284              12     1.0         1.0         1.0   \n",
       "\n",
       "   target_mean  target_shop  target_item  Expanding Mean  \\\n",
       "0     0.000000       8198.0         23.0        0.483516   \n",
       "1     0.000000       8198.0         17.0        0.521739   \n",
       "2     0.881917       8198.0        182.0       13.299270   \n",
       "3     0.000000       8198.0         26.0        3.416058   \n",
       "4     0.000000       8198.0          3.0        0.133772   \n",
       "\n",
       "   Expanding Mean_lag_1  target_lag_1  target_item_lag_1  target_max_lag_1  \\\n",
       "0              0.408451           3.0               42.0               1.0   \n",
       "1              0.615385           0.0               24.0               0.0   \n",
       "2             14.136752          21.0              369.0               3.0   \n",
       "3              3.829060           1.0               54.0               1.0   \n",
       "4              0.135321           0.0                4.0               0.0   \n",
       "\n",
       "   target_mean_lag_1  target_min_lag_1  target_shop_lag_1  \\\n",
       "0           0.000000               1.0            10055.0   \n",
       "1           0.000000               0.0            10055.0   \n",
       "2           0.759555               1.0            10055.0   \n",
       "3           0.000000               1.0            10055.0   \n",
       "4           0.000000               0.0            10055.0   \n",
       "\n",
       "   Expanding Mean_lag_2  target_lag_2  target_item_lag_2  target_max_lag_2  \\\n",
       "0              0.064516           0.0                2.0               0.0   \n",
       "1              0.000000           0.0                0.0               0.0   \n",
       "2             11.935065         119.0             1309.0              15.0   \n",
       "3              3.727273          31.0              361.0               5.0   \n",
       "4              0.138889           0.0                3.0               0.0   \n",
       "\n",
       "   target_mean_lag_2  target_min_lag_2  target_shop_lag_2  \\\n",
       "0           0.000000               0.0             7978.0   \n",
       "1           0.000000               0.0                0.0   \n",
       "2           4.582378               1.0             7978.0   \n",
       "3           1.423893               1.0             7978.0   \n",
       "4           0.000000               0.0             7978.0   \n",
       "\n",
       "   Expanding Mean_lag_3        ...         target_mean_lag_3  \\\n",
       "0              0.000000        ...                   0.00000   \n",
       "1              0.000000        ...                   0.00000   \n",
       "2              3.166667        ...                   2.12132   \n",
       "3              0.944444        ...                   0.00000   \n",
       "4              0.151335        ...                   0.00000   \n",
       "\n",
       "   target_min_lag_3  target_shop_lag_3  Expanding Mean_lag_4  target_lag_4  \\\n",
       "0               0.0                0.0              0.000000           0.0   \n",
       "1               0.0                0.0              0.000000           0.0   \n",
       "2               2.0             6676.0              0.000000           0.0   \n",
       "3               0.0             6676.0              0.000000           0.0   \n",
       "4               0.0             6676.0              0.166065           0.0   \n",
       "\n",
       "   target_item_lag_4  target_max_lag_4  target_mean_lag_4  target_min_lag_4  \\\n",
       "0                0.0               0.0                0.0               0.0   \n",
       "1                0.0               0.0                0.0               0.0   \n",
       "2                0.0               0.0                0.0               0.0   \n",
       "3                0.0               0.0                0.0               0.0   \n",
       "4                3.0               0.0                0.0               0.0   \n",
       "\n",
       "   target_shop_lag_4  Expanding Mean_lag_5  target_lag_5  target_item_lag_5  \\\n",
       "0                0.0              0.000000           0.0                0.0   \n",
       "1                0.0              0.000000           0.0                0.0   \n",
       "2                0.0              0.000000           0.0                0.0   \n",
       "3                0.0              0.000000           0.0                0.0   \n",
       "4             7827.0              0.160173           0.0               10.0   \n",
       "\n",
       "   target_max_lag_5  target_mean_lag_5  target_min_lag_5  target_shop_lag_5  \\\n",
       "0               0.0                0.0               0.0                0.0   \n",
       "1               0.0                0.0               0.0                0.0   \n",
       "2               0.0                0.0               0.0                0.0   \n",
       "3               0.0                0.0               0.0                0.0   \n",
       "4               0.0                0.0               0.0             7792.0   \n",
       "\n",
       "   Expanding Mean_lag_12  target_lag_12  target_item_lag_12  \\\n",
       "0                    0.0            0.0                 0.0   \n",
       "1                    0.0            0.0                 0.0   \n",
       "2                    0.0            0.0                 0.0   \n",
       "3                    0.0            0.0                 0.0   \n",
       "4                    0.0            0.0                 0.0   \n",
       "\n",
       "   target_max_lag_12  target_mean_lag_12  target_min_lag_12  \\\n",
       "0                0.0                 0.0                0.0   \n",
       "1                0.0                 0.0                0.0   \n",
       "2                0.0                 0.0                0.0   \n",
       "3                0.0                 0.0                0.0   \n",
       "4                0.0                 0.0                0.0   \n",
       "\n",
       "   target_shop_lag_12  item_category_id  \n",
       "0                 0.0                37  \n",
       "1                 0.0                38  \n",
       "2                 0.0                40  \n",
       "3                 0.0                37  \n",
       "4                 0.0                57  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test. We will treat last month data as the test set. `Test` should really be called `validation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 33\n",
      "Submit `date_block_num` is 34\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "sub_block = dates.max()\n",
    "last_block = dates.max() - 1\n",
    "print('Test `date_block_num` is %d' % last_block)\n",
    "print('Submit `date_block_num` is %d' % sub_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train = dates[dates <  last_block]\n",
    "dates_test  = dates[dates == last_block]\n",
    "\n",
    "X_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n",
    "X_sub  =  all_data.loc[dates == sub_block].drop(to_drop_cols, axis=1)\n",
    "\n",
    "\n",
    "y_train = all_data.loc[dates <  last_block, 'target'].values\n",
    "y_test =  all_data.loc[dates == last_block, 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6186922, 45)\n",
      "(238172, 45)\n",
      "(214200, 45)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "\n",
    "print (X_test.shape)\n",
    "\n",
    "print (X_sub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First level models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at 3 different linear regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for linreg is 0.963864\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values, y_train)\n",
    "pred_lr = lr.predict(X_test.values)\n",
    "\n",
    "pred_lr = pred_lr.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print('Test RMSE for linreg is %f' % mean_squared_error(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for lassoReg is 0.991678\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lassoReg = Lasso(alpha=0.8)\n",
    "lassoReg.fit(X_train.values, y_train) \n",
    "pred_lasso = lassoReg.predict(X_test.values)\n",
    "\n",
    "pred_lasso = pred_lasso.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print('Test RMSE for lassoReg is %f' % mean_squared_error(y_test, pred_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=True, random_state=None, solver='auto', tol=0.001)\n",
      "Test RMSE for ridgeReg is 0.971632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgeReg = Ridge(alpha=0.5,normalize=True )\n",
    "\n",
    "print (ridgeReg)\n",
    "ridgeReg.fit(X_train.values, y_train) \n",
    "pred_ridge = ridgeReg.predict(X_test.values)\n",
    "\n",
    "pred_ridge = pred_ridge.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print('Test RMSE for ridgeReg is %f' % mean_squared_error(y_test, pred_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the hyperparameters for LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_fraction is 0.400000\n",
      "Test RMSE for LightGBM is 0.935499\n",
      "feature_fraction is 0.433333\n",
      "Test RMSE for LightGBM is 0.934284\n",
      "feature_fraction is 0.466667\n",
      "Test RMSE for LightGBM is 0.906183\n",
      "feature_fraction is 0.500000\n",
      "Test RMSE for LightGBM is 0.904924\n",
      "feature_fraction is 0.533333\n",
      "Test RMSE for LightGBM is 0.914229\n",
      "feature_fraction is 0.566667\n",
      "Test RMSE for LightGBM is 0.934948\n",
      "feature_fraction is 0.600000\n",
      "Test RMSE for LightGBM is 0.950086\n",
      "feature_fraction is 0.633333\n",
      "Test RMSE for LightGBM is 0.961090\n",
      "feature_fraction is 0.666667\n",
      "Test RMSE for LightGBM is 0.979925\n",
      "feature_fraction is 0.700000\n",
      "Test RMSE for LightGBM is 0.999587\n"
     ]
    }
   ],
   "source": [
    "ffs_to_try = np.linspace(0.4, 0.7, 10)\n",
    "\n",
    "for ff in ffs_to_try:\n",
    "\n",
    "    print('feature_fraction is %f' % ff)\n",
    "\n",
    "\n",
    "    lgb_params = {\n",
    "               'feature_fraction': ff,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':8, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.01, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "    model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "    pred_lgb = model.predict(X_test)\n",
    "\n",
    "    pred_lgb = pred_lgb.clip(0,20.)\n",
    "    y_test = y_test.clip(0,20.)\n",
    "    print('Test RMSE for LightGBM is %f' % mean_squared_error(y_test, pred_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for LightGBM is 0.904924\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "               'feature_fraction': 0.5,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':8, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.01, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "pred_lgb = model.predict(X_test)\n",
    "\n",
    "pred_lgb = pred_lgb.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "print('Test RMSE for LightGBM is %f' % mean_squared_error(y_test, pred_lgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenate test predictions to get test meta-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238172, 2)\n"
     ]
    }
   ],
   "source": [
    "X_test_level2 = np.c_[pred_lr, pred_lgb] \n",
    "\n",
    "\n",
    "print (X_test_level2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-Features for the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214200, 45)\n",
      "(214200, 2)\n"
     ]
    }
   ],
   "source": [
    "print (X_sub.shape)\n",
    "\n",
    "sub_lr = lr.predict(X_sub.values).clip(0,20.)\n",
    "#sub_lr = lassoReg.predict(X_sub.values).clip(0,20.)\n",
    "sub_lgb = model.predict(X_sub).clip(0,20.)\n",
    "\n",
    "X_sub_level2 = np.c_[sub_lr, sub_lgb] \n",
    "print (X_sub_level2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement ***scheme f)*** from the reading material. Here, we will use duration **T** equal to month and **M=15**.  \n",
    "\n",
    "We need to get predictions (meta-features) from *linear regression* and *LightGBM* for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "y_train_level2 = y_train_level2.clip(0,20.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n",
    "\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in [27, 28, 29, 30, 31, 32]:\n",
    "    \n",
    "    print(cur_block_num)\n",
    "    \n",
    "    Xtemp = X_train[dates_train < cur_block_num]\n",
    "    ytemp = y_train[dates_train < cur_block_num]\n",
    "    \n",
    "    Xtemp_1 = X_train[dates_train == cur_block_num]\n",
    "    \n",
    "    lr.fit(Xtemp.values, ytemp)\n",
    "    pred_lr = lr.predict(Xtemp_1.values)\n",
    "    pred_lr = pred_lr.clip(0,20.)\n",
    "    \n",
    "    model = lgb.train(lgb_params, lgb.Dataset(Xtemp, label=ytemp), 100)\n",
    "    pred_lgb = model.predict(Xtemp_1)\n",
    "    pred_lgb = pred_lgb.clip(0,20.)\n",
    "\n",
    "\n",
    "    #X_train_level2[dates_train_level2==cur_block_num,:2] = np.c_[pred_lr, pred_lgb]\n",
    "    X_train_level2[dates_train_level2==cur_block_num,:] = np.c_[pred_lr, pred_lgb]\n",
    "\n",
    "#print (X_train_level2.shape)\n",
    "#print(X_train_level2.mean(axis=0))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure there is sufficient diversity between the 2 meta models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsILfYrFYOgwr+C0Wi6XD+P8BkC0YVhdcluUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c2b035400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_train_level2[:,0], X_train_level2[:,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the meta-features are created, we can ensemble our first level models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple linear convex mix:\n",
    "\n",
    "$$\n",
    "mix= \\alpha\\cdot\\text{linreg_prediction}+(1-\\alpha)\\cdot\\text{lgb_prediction}\n",
    "$$\n",
    "\n",
    "We need to find an optimal $\\alpha$ vy doing a grid search. Find the optimal $\\alpha$ out of `alphas_to_try` array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the training error for simple convex mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.315000; Corresponding RMSE score on train: 0.726261\n"
     ]
    }
   ],
   "source": [
    "alphas_to_try = np.linspace(0, 1, 1001)\n",
    "\n",
    "r2_train_simple_mix = 2.0\n",
    "\n",
    "for alpha in alphas_to_try:\n",
    "    mix = alpha*X_train_level2[:,0] + (1.0-alpha)*X_train_level2[:,1]   \n",
    "    mix = mix.clip(0,20.)\n",
    "    #print (np.amax(y_train_level2 - mix))\n",
    "\n",
    "    rtemp = mean_squared_error(y_train_level2, mix)\n",
    "    if (rtemp < r2_train_simple_mix) :\n",
    "        r2_train_simple_mix = rtemp\n",
    "        best_alpha = alpha\n",
    "        \n",
    "\n",
    "print('Best alpha: %f; Corresponding RMSE score on train: %f' % (best_alpha, r2_train_simple_mix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the testing (validation) error for simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the best $\\alpha$ to compute predictions for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238172, 2)\n",
      "(238172,)\n",
      "(238172,)\n",
      "Test RMSE for simple mix is 0.884330\n"
     ]
    }
   ],
   "source": [
    "test_preds = best_alpha*X_test_level2[:,0]  +  (1.0-best_alpha)*X_test_level2[:,1]\n",
    "\n",
    "test_preds = test_preds.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print (X_test_level2.shape)\n",
    "print (y_test.shape)\n",
    "print (test_preds.shape)\n",
    "\n",
    "r2_test_simple_mix = mean_squared_error(y_test, test_preds)     # YOUR CODE GOES HERE\n",
    "\n",
    "print('Test RMSE for simple mix is %f' % r2_test_simple_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear regression model to the meta-features. Use the same parameters as in the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "linReg = LinearRegression(normalize=False)\n",
    "linReg.fit(X_train_level2, y_train_level2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute RSME on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE for stacking is 0.715539\n",
      "Test  RMSE for stacking is 0.877475\n"
     ]
    }
   ],
   "source": [
    "train_preds = linReg.predict(X_train_level2)  \n",
    "train_preds = train_preds.clip(0,20.)                                       \n",
    "r2_train_stacking = mean_squared_error(y_train_level2, train_preds)     \n",
    "\n",
    "test_preds = linReg.predict(X_test_level2)              \n",
    "test_preds = test_preds.clip(0,20.)                                     \n",
    "\n",
    "r2_test_stacking = mean_squared_error(y_test, test_preds)   \n",
    "\n",
    "print('Train RMSE for stacking is %f' % r2_train_stacking)\n",
    "print('Test  RMSE for stacking is %f' % r2_test_stacking)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking method produced a lower Test RMSE than simple mix method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214200, 45)\n",
      "(214200,)\n",
      "(214200, 3)\n",
      "(214200, 47)\n"
     ]
    }
   ],
   "source": [
    "# submission using the stacking method\n",
    "sub_preds = linReg.predict(X_sub_level2)              \n",
    "sub_preds = sub_preds.clip(0,20.)  \n",
    "\n",
    "\n",
    "# submission using the simple mix method\n",
    "#sub_preds = best_alpha*X_sub_level2[:,0]  +  (1.0-best_alpha)*X_sub_level2[:,1]\n",
    "#sub_preds = sub_preds.clip(0,20.)\n",
    "\n",
    "\n",
    "print (X_sub.shape)\n",
    "print (sub_preds.shape)\n",
    "\n",
    "X_sub['target'] = sub_preds\n",
    "\n",
    "ww = pd.read_csv('../Final Project/test.csv.gz')\n",
    "\n",
    "\n",
    "submit = pd.merge(ww,X_sub,how='left',on=['shop_id', 'item_id']).fillna(0)\n",
    "\n",
    "print (ww.shape)\n",
    "print (submit.shape)\n",
    "\n",
    "#print (submit.head(10))\n",
    "#print (submit[submit.date_block_num == 0.0].count())\n",
    "\n",
    "submit.to_csv(path_or_buf='submit_max_min.std_Mean_Encoding_stacking.csv',columns=['ID','target'], header=['ID', 'item_cnt_month'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
