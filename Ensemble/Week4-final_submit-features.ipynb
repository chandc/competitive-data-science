{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check software versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.13.3\n",
      "pandas 0.20.3\n",
      "scipy 0.19.1\n",
      "sklearn 0.19.1\n",
      "lightgbm 2.0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "import lightgbm \n",
    "\n",
    "for p in [np, pd, scipy, sklearn, lightgbm]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv('../Final Project/sales_train.csv.gz')\n",
    "shops = pd.read_csv('../Final Project/shops.csv')\n",
    "items = pd.read_csv('../Final Project/items.csv')\n",
    "item_cats = pd.read_csv('../Final Project/item_categories.csv')\n",
    "ww = pd.read_csv('../Final Project/test.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concatenate `test` file to the end of `sales` dataframe so we can calculate meta-features consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  shop_id  item_id\n",
      "0   0        5     5037\n",
      "1   1        5     5320\n",
      "2   2        5     5233\n",
      "3   3        5     5232\n",
      "4   4        5     5268\n"
     ]
    }
   ],
   "source": [
    "#print (sales.head(5))\n",
    "print (ww.head(5))\n",
    "del sales['date'] \n",
    "del sales['item_price']\n",
    "del ww['ID']\n",
    "ww['item_cnt_day'] = 0\n",
    "ww['date_block_num'] = 34\n",
    "\n",
    "sales = pd.concat([sales,ww],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielchan/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# additional features derived from sales statistics: max, min and std \n",
    "\n",
    "# Same as above but with shop-item-month max\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target_max':np.max}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-item-month min\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target_min':np.min}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-item-month mean\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target_mean':np.mean}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Expanding Mean Encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499605291823\n"
     ]
    }
   ],
   "source": [
    "globalmean = all_data['target'].mean()\n",
    "\n",
    "csum   = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\n",
    "cumcnt = all_data.groupby('item_id').cumcount()\n",
    "\n",
    "small = 0.\n",
    "\n",
    "all_data['Expanding Mean'] = csum/(cumcnt+small)\n",
    "all_data['Expanding Mean'].fillna(globalmean, inplace=True)    # Fill NaNs with global mean\n",
    "\n",
    "encoded_feature = all_data['Expanding Mean'].values\n",
    "\n",
    "\n",
    "corr = np.corrcoef(all_data['target'].values, encoded_feature)[0][1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11128050, 10)\n"
     ]
    }
   ],
   "source": [
    "print (all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f456130eef40bdaafdd38b490ba7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the `all_data` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_max</th>\n",
       "      <th>target_min</th>\n",
       "      <th>target_mean</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>Expanding Mean</th>\n",
       "      <th>Expanding Mean_lag_1</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_max_lag_1</th>\n",
       "      <th>target_mean_lag_1</th>\n",
       "      <th>target_min_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>Expanding Mean_lag_2</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_max_lag_2</th>\n",
       "      <th>target_mean_lag_2</th>\n",
       "      <th>target_min_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>Expanding Mean_lag_3</th>\n",
       "      <th>...</th>\n",
       "      <th>target_mean_lag_3</th>\n",
       "      <th>target_min_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>Expanding Mean_lag_4</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_max_lag_4</th>\n",
       "      <th>target_mean_lag_4</th>\n",
       "      <th>target_min_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>Expanding Mean_lag_5</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_max_lag_5</th>\n",
       "      <th>target_mean_lag_5</th>\n",
       "      <th>target_min_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>Expanding Mean_lag_12</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_max_lag_12</th>\n",
       "      <th>target_mean_lag_12</th>\n",
       "      <th>target_min_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>10297</td>\n",
       "      <td>12</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.483516</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>3.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>10296</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>10298</td>\n",
       "      <td>12</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>13.299270</td>\n",
       "      <td>14.136752</td>\n",
       "      <td>21.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>11.935065</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1309.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6676.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>10300</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.416058</td>\n",
       "      <td>3.829060</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>3.727273</td>\n",
       "      <td>31.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.214286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6676.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>10284</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.133772</td>\n",
       "      <td>0.135321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.151335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6676.0</td>\n",
       "      <td>0.166065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7827.0</td>\n",
       "      <td>0.160173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7792.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  target  target_max  target_min  \\\n",
       "0       54    10297              12     4.0         1.0         1.0   \n",
       "1       54    10296              12     3.0         1.0         1.0   \n",
       "2       54    10298              12    14.0         3.0         1.0   \n",
       "3       54    10300              12     3.0         1.0         1.0   \n",
       "4       54    10284              12     1.0         1.0         1.0   \n",
       "\n",
       "   target_mean  target_shop  target_item  Expanding Mean  \\\n",
       "0     1.000000       8198.0         23.0        0.483516   \n",
       "1     1.000000       8198.0         17.0        0.521739   \n",
       "2     1.555556       8198.0        182.0       13.299270   \n",
       "3     1.000000       8198.0         26.0        3.416058   \n",
       "4     1.000000       8198.0          3.0        0.133772   \n",
       "\n",
       "   Expanding Mean_lag_1  target_lag_1  target_item_lag_1  target_max_lag_1  \\\n",
       "0              0.408451           3.0               42.0               1.0   \n",
       "1              0.615385           0.0               24.0               0.0   \n",
       "2             14.136752          21.0              369.0               3.0   \n",
       "3              3.829060           1.0               54.0               1.0   \n",
       "4              0.135321           0.0                4.0               0.0   \n",
       "\n",
       "   target_mean_lag_1  target_min_lag_1  target_shop_lag_1  \\\n",
       "0                1.0               1.0            10055.0   \n",
       "1                0.0               0.0            10055.0   \n",
       "2                1.5               1.0            10055.0   \n",
       "3                1.0               1.0            10055.0   \n",
       "4                0.0               0.0            10055.0   \n",
       "\n",
       "   Expanding Mean_lag_2  target_lag_2  target_item_lag_2  target_max_lag_2  \\\n",
       "0              0.064516           0.0                2.0               0.0   \n",
       "1              0.000000           0.0                0.0               0.0   \n",
       "2             11.935065         119.0             1309.0              15.0   \n",
       "3              3.727273          31.0              361.0               5.0   \n",
       "4              0.138889           0.0                3.0               0.0   \n",
       "\n",
       "   target_mean_lag_2  target_min_lag_2  target_shop_lag_2  \\\n",
       "0           0.000000               0.0             7978.0   \n",
       "1           0.000000               0.0                0.0   \n",
       "2           4.958333               1.0             7978.0   \n",
       "3           2.214286               1.0             7978.0   \n",
       "4           0.000000               0.0             7978.0   \n",
       "\n",
       "   Expanding Mean_lag_3        ...         target_mean_lag_3  \\\n",
       "0              0.000000        ...                       0.0   \n",
       "1              0.000000        ...                       0.0   \n",
       "2              3.166667        ...                       3.5   \n",
       "3              0.944444        ...                       0.0   \n",
       "4              0.151335        ...                       0.0   \n",
       "\n",
       "   target_min_lag_3  target_shop_lag_3  Expanding Mean_lag_4  target_lag_4  \\\n",
       "0               0.0                0.0              0.000000           0.0   \n",
       "1               0.0                0.0              0.000000           0.0   \n",
       "2               2.0             6676.0              0.000000           0.0   \n",
       "3               0.0             6676.0              0.000000           0.0   \n",
       "4               0.0             6676.0              0.166065           0.0   \n",
       "\n",
       "   target_item_lag_4  target_max_lag_4  target_mean_lag_4  target_min_lag_4  \\\n",
       "0                0.0               0.0                0.0               0.0   \n",
       "1                0.0               0.0                0.0               0.0   \n",
       "2                0.0               0.0                0.0               0.0   \n",
       "3                0.0               0.0                0.0               0.0   \n",
       "4                3.0               0.0                0.0               0.0   \n",
       "\n",
       "   target_shop_lag_4  Expanding Mean_lag_5  target_lag_5  target_item_lag_5  \\\n",
       "0                0.0              0.000000           0.0                0.0   \n",
       "1                0.0              0.000000           0.0                0.0   \n",
       "2                0.0              0.000000           0.0                0.0   \n",
       "3                0.0              0.000000           0.0                0.0   \n",
       "4             7827.0              0.160173           0.0               10.0   \n",
       "\n",
       "   target_max_lag_5  target_mean_lag_5  target_min_lag_5  target_shop_lag_5  \\\n",
       "0               0.0                0.0               0.0                0.0   \n",
       "1               0.0                0.0               0.0                0.0   \n",
       "2               0.0                0.0               0.0                0.0   \n",
       "3               0.0                0.0               0.0                0.0   \n",
       "4               0.0                0.0               0.0             7792.0   \n",
       "\n",
       "   Expanding Mean_lag_12  target_lag_12  target_item_lag_12  \\\n",
       "0                    0.0            0.0                 0.0   \n",
       "1                    0.0            0.0                 0.0   \n",
       "2                    0.0            0.0                 0.0   \n",
       "3                    0.0            0.0                 0.0   \n",
       "4                    0.0            0.0                 0.0   \n",
       "\n",
       "   target_max_lag_12  target_mean_lag_12  target_min_lag_12  \\\n",
       "0                0.0                 0.0                0.0   \n",
       "1                0.0                 0.0                0.0   \n",
       "2                0.0                 0.0                0.0   \n",
       "3                0.0                 0.0                0.0   \n",
       "4                0.0                 0.0                0.0   \n",
       "\n",
       "   target_shop_lag_12  item_category_id  \n",
       "0                 0.0                37  \n",
       "1                 0.0                38  \n",
       "2                 0.0                40  \n",
       "3                 0.0                37  \n",
       "4                 0.0                57  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test. We will treat last month data as the test set. Test should really be called validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 33\n",
      "Submit `date_block_num` is 34\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "sub_block = dates.max()\n",
    "last_block = dates.max() - 1\n",
    "print('Test `date_block_num` is %d' % last_block)\n",
    "print('Submit `date_block_num` is %d' % sub_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train = dates[dates <  last_block]\n",
    "dates_test  = dates[dates == last_block]\n",
    "\n",
    "X_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n",
    "X_sub  =  all_data.loc[dates == sub_block].drop(to_drop_cols, axis=1)\n",
    "\n",
    "\n",
    "y_train = all_data.loc[dates <  last_block, 'target'].values\n",
    "y_test =  all_data.loc[dates == last_block, 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6186922, 45)\n",
      "(238172, 45)\n",
      "(214200, 45)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "\n",
    "print (X_test.shape)\n",
    "\n",
    "print (X_sub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First level models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test meta-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for linreg is 0.962656\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values, y_train)\n",
    "pred_lr = lr.predict(X_test.values)\n",
    "\n",
    "pred_lr = pred_lr.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print('Test RMSE for linreg is %f' % mean_squared_error(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for lassoReg is 0.991678\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lassoReg = Lasso(alpha=0.8)\n",
    "lassoReg.fit(X_train.values, y_train) \n",
    "pred_lasso = lassoReg.predict(X_test.values)\n",
    "\n",
    "pred_lasso = pred_lasso.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print('Test RMSE for lassoReg is %f' % mean_squared_error(y_test, pred_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=True, random_state=None, solver='auto', tol=0.001)\n",
      "Test RMSE for ridgeReg is 0.976170\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgeReg = Ridge(alpha=0.5,normalize=True )\n",
    "\n",
    "print (ridgeReg)\n",
    "ridgeReg.fit(X_train.values, y_train) \n",
    "pred_ridge = ridgeReg.predict(X_test.values)\n",
    "\n",
    "pred_ridge = pred_ridge.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print('Test RMSE for ridgeReg is %f' % mean_squared_error(y_test, pred_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for LightGBM is 0.906805\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "               'feature_fraction': 0.5,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':8, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.01, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "pred_lgb = model.predict(X_test)\n",
    "\n",
    "pred_lgb = pred_lgb.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print('Test RMSE for LightGBM is %f' % mean_squared_error(y_test, pred_lgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenate test predictions to get test meta-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238172, 2)\n"
     ]
    }
   ],
   "source": [
    "X_test_level2 = np.c_[pred_lr, pred_lgb] \n",
    "\n",
    "\n",
    "print (X_test_level2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-Features for the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214200, 45)\n",
      "(214200, 2)\n"
     ]
    }
   ],
   "source": [
    "print (X_sub.shape)\n",
    "\n",
    "sub_lr = lr.predict(X_sub.values).clip(0,20.)\n",
    "#sub_lr = lassoReg.predict(X_sub.values).clip(0,20.)\n",
    "sub_lgb = model.predict(X_sub).clip(0,20.)\n",
    "\n",
    "X_sub_level2 = np.c_[sub_lr, sub_lgb] \n",
    "print (X_sub_level2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement ***scheme f)*** from the reading material. Here, we will use duration **T** equal to month and **M=15**.  \n",
    "\n",
    "We need to get predictions (meta-features) from *linear regression* and *LightGBM* for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "y_train_level2 = y_train_level2.clip(0,20.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n",
    "\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in [27, 28, 29, 30, 31, 32]:\n",
    "    \n",
    "    print(cur_block_num)\n",
    "    \n",
    "    Xtemp = X_train[dates_train < cur_block_num]\n",
    "    ytemp = y_train[dates_train < cur_block_num]\n",
    "    \n",
    "    Xtemp_1 = X_train[dates_train == cur_block_num]\n",
    "    \n",
    "    lr.fit(Xtemp.values, ytemp)\n",
    "    pred_lr = lr.predict(Xtemp_1.values)\n",
    "    pred_lr = pred_lr.clip(0,20.)\n",
    "    \n",
    "    model = lgb.train(lgb_params, lgb.Dataset(Xtemp, label=ytemp), 100)\n",
    "    pred_lgb = model.predict(Xtemp_1)\n",
    "    pred_lgb = pred_lgb.clip(0,20.)\n",
    "\n",
    "\n",
    "    #X_train_level2[dates_train_level2==cur_block_num,:2] = np.c_[pred_lr, pred_lgb]\n",
    "    X_train_level2[dates_train_level2==cur_block_num,:] = np.c_[pred_lr, pred_lgb]\n",
    "\n",
    "#print (X_train_level2.shape)\n",
    "#print(X_train_level2.mean(axis=0))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure there is sufficient diversity between the 2 meta models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+C0Wi6XLsILfYrFYuoz/H2I9ZG/EVLfzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7c409b9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_train_level2[:,0], X_train_level2[:,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the meta-features are created, we can ensemble our first level models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple linear convex mix:\n",
    "\n",
    "$$\n",
    "mix= \\alpha\\cdot\\text{linreg_prediction}+(1-\\alpha)\\cdot\\text{lgb_prediction}\n",
    "$$\n",
    "\n",
    "We need to find an optimal $\\alpha$ vy doing a grid search. Find the optimal $\\alpha$ out of `alphas_to_try` array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.322000; Corresponding RMSE score on train: 0.727047\n"
     ]
    }
   ],
   "source": [
    "alphas_to_try = np.linspace(0, 1, 1001)\n",
    "\n",
    "r2_train_simple_mix = 2.0\n",
    "\n",
    "for alpha in alphas_to_try:\n",
    "    mix = alpha*X_train_level2[:,0] + (1.0-alpha)*X_train_level2[:,1]   \n",
    "    mix = mix.clip(0,20.)\n",
    "    #print (np.amax(y_train_level2 - mix))\n",
    "\n",
    "    rtemp = mean_squared_error(y_train_level2, mix)\n",
    "    if (rtemp < r2_train_simple_mix) :\n",
    "        r2_train_simple_mix = rtemp\n",
    "        best_alpha = alpha\n",
    "        \n",
    "\n",
    "print('Best alpha: %f; Corresponding RMSE score on train: %f' % (best_alpha, r2_train_simple_mix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the best $\\alpha$ to compute predictions for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238172, 2)\n",
      "(238172,)\n",
      "(238172,)\n",
      "Test RMSE for simple mix is 0.884453\n"
     ]
    }
   ],
   "source": [
    "test_preds = best_alpha*X_test_level2[:,0]  +  (1.0-best_alpha)*X_test_level2[:,1]\n",
    "\n",
    "test_preds = test_preds.clip(0,20.)\n",
    "y_test = y_test.clip(0,20.)\n",
    "\n",
    "print (X_test_level2.shape)\n",
    "print (y_test.shape)\n",
    "print (test_preds.shape)\n",
    "\n",
    "r2_test_simple_mix = mean_squared_error(y_test, test_preds)     # YOUR CODE GOES HERE\n",
    "\n",
    "print('Test RMSE for simple mix is %f' % r2_test_simple_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear regression model to the meta-features. Use the same parameters as in the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "linReg = LinearRegression(normalize=False)\n",
    "linReg.fit(X_train_level2, y_train_level2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute RSME on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE for stacking is 0.716274\n",
      "Test  RMSE for stacking is 0.877629\n"
     ]
    }
   ],
   "source": [
    "train_preds = linReg.predict(X_train_level2)  \n",
    "train_preds = train_preds.clip(0,20.)                                       \n",
    "r2_train_stacking = mean_squared_error(y_train_level2, train_preds)     \n",
    "\n",
    "test_preds = linReg.predict(X_test_level2)              \n",
    "test_preds = test_preds.clip(0,20.)                                     \n",
    "\n",
    "r2_test_stacking = mean_squared_error(y_test, test_preds)   \n",
    "\n",
    "print('Train RMSE for stacking is %f' % r2_train_stacking)\n",
    "print('Test  RMSE for stacking is %f' % r2_test_stacking)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking method produced a lower Test RMSE than simple mix method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214200, 45)\n",
      "(214200,)\n",
      "(214200, 3)\n",
      "(214200, 47)\n"
     ]
    }
   ],
   "source": [
    "sub_preds = linReg.predict(X_sub_level2)              \n",
    "sub_preds = sub_preds.clip(0,20.)  \n",
    "\n",
    "\n",
    "# submission using the simple mix method\n",
    "#sub_preds = best_alpha*X_sub_level2[:,0]  +  (1.0-best_alpha)*X_sub_level2[:,1]\n",
    "#sub_preds = sub_preds.clip(0,20.)\n",
    "\n",
    "\n",
    "print (X_sub.shape)\n",
    "print (sub_preds.shape)\n",
    "\n",
    "X_sub['target'] = sub_preds\n",
    "\n",
    "ww = pd.read_csv('../Final Project/test.csv.gz')\n",
    "\n",
    "\n",
    "submit = pd.merge(ww,X_sub,how='left',on=['shop_id', 'item_id']).fillna(0)\n",
    "\n",
    "print (ww.shape)\n",
    "print (submit.shape)\n",
    "\n",
    "#print (submit.head(10))\n",
    "#print (submit[submit.date_block_num == 0.0].count())\n",
    "\n",
    "submit.to_csv(path_or_buf='submit_max_min.mean_Mean_Encoding_stacking.csv',columns=['ID','target'], header=['ID', 'item_cnt_month'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
